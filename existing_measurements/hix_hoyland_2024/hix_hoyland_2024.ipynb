{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recreate Hix Hoyland (2024)\n",
        "This notebook is based on the script `eu-policy-feedback/existing_measurements/hix_hoyland_2024/hix_hoyland_2024.R` and is optimised to process a large amount (>70k) of EU legislations using Google Colab hardware, e.g., a GPU.\n",
        "\n",
        "\n",
        "Connect to a GPU for best performance."
      ],
      "metadata": {
        "id": "9JsbDJ33WOyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "jEu7c2gvXLN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets tqdm"
      ],
      "metadata": {
        "id": "IBklQmbvPChG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from itertools import islice\n",
        "import math\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from datasets import Dataset\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import random\n",
        "\n",
        "# Download NLTK stopwords and punkt tokenizer if you haven't already\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "-uJFI05QFkKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t3MK1cvYE6Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "id": "ErXV9kWoWd77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_dir_reg = pd.read_csv(\"/content/drive/MyDrive/EU Policy Feedback/all_dir_reg.csv\")\n",
        "#all_dir_reg = pd.read_csv(\"/content/drive/MyDrive/EU Policy Feedback/all_dir_reg_sample.csv\")\n",
        "all_dir_reg = all_dir_reg.sample(n=5000, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "eomh3V_6EzLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing\n",
        "\"For each piece of legislation, we classified each sentence in the preamble, until the phrase “Adopted this directive/regulation”, using a RoBERT-classifier trained on the corpus of party manifestos\"\n",
        "\n",
        "Get preamble string until “Adopted this directive/regulation”"
      ],
      "metadata": {
        "id": "LuhbcR-zWWck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract the preamble text\n",
        "def extract_preamble(text):\n",
        "    if not isinstance(text, str):\n",
        "        return None  # or you could return \"\" if you prefer an empty string\n",
        "\n",
        "    # Use a case-insensitive regex to find the first occurrence of the keywords\n",
        "    match = re.search(r'(?i)(Adopted this directive|Adopted this regulation)', text)\n",
        "\n",
        "    # If the keyword is found, truncate the string\n",
        "    if match:\n",
        "        return text[:match.start()]\n",
        "    else:\n",
        "        return text"
      ],
      "metadata": {
        "id": "Pk6ZehmDW25a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming all_dir_reg is a pandas DataFrame and 'act_raw_text' is the column with text\n",
        "all_dir_reg['preamble'] = all_dir_reg['act_raw_text'].apply(extract_preamble)\n"
      ],
      "metadata": {
        "id": "Dxc2xquZFh2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional Preprocessing (Appropriate for word embedding)"
      ],
      "metadata": {
        "id": "nHDOFL4QPvoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the text\n",
        "\n",
        "# Custom function to process each segment\n",
        "def process_text(text, procedural_stop_words):\n",
        "    # Check if the input is a string\n",
        "    if not isinstance(text, str):\n",
        "        return text  # If not a string, return the original input\n",
        "\n",
        "    # Tokenize and remove unwanted characters\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove punctuations, symbols, numbers, and URLs\n",
        "    tokens = [token for token in tokens if token.isalnum()]\n",
        "\n",
        "    # Remove stopwords (you can replace 'marimo' with another source if needed)\n",
        "    marimo_stopwords = set(stopwords.words('english'))  # Assuming marimo is similar to 'english' stopwords\n",
        "    tokens = [token for token in tokens if token.lower() not in marimo_stopwords]\n",
        "\n",
        "    # Remove corpus-specific irrelevant words\n",
        "    tokens = [token for token in tokens if token.lower() not in procedural_stop_words]\n",
        "\n",
        "    # Remove mixed letter-number tokens\n",
        "    tokens = [token for token in tokens if not re.match(r'\\b(?=\\w*[A-Za-z])(?=\\w*\\d)\\w+\\b', token)]\n",
        "\n",
        "    # Remove mixed letter-punctuation tokens\n",
        "    tokens = [token for token in tokens if not re.match(r'\\b(?=.*\\d)(?=.*[{}])\\S+\\b'.format(re.escape(string.punctuation)), token)]\n",
        "\n",
        "    # Remove tokens shorter than 3 characters\n",
        "    tokens = [token for token in tokens if len(token) >= 3]\n",
        "\n",
        "    # Remove tokens that are sequences of numbers possibly separated by slashes\n",
        "    tokens = [token for token in tokens if not re.match(r'^\\d+(/\\d+)*$', token)]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "procedural_stop_words = [\"article*\", \"shall\", \"annex\", \"commission\", \"decision\", \"member\", \"european\", \"state*\", \"measure*\", \"regard\", \"directive\", \"ii\", \"iii\", \"first\", \"second\", \"third\", \"fourth\", \"1st\", \"2nd\", \"3rd\", \"4th\", \"thereof\", \"act*\", \"add*\", \"adopt*\", \"also\", \"dateformat\"]\n",
        "\n",
        "# Check if the 'preamble' column exists in the dataframe\n",
        "if 'preamble' in all_dir_reg.columns:\n",
        "    # Apply the process_text function to the 'preamble' column\n",
        "    all_dir_reg['preamble'] = all_dir_reg['preamble'].apply(lambda x: process_text(x, procedural_stop_words))\n"
      ],
      "metadata": {
        "id": "NdCld9wWPxIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Subsampling\n",
        "\n",
        "# Assuming 'all_dir_reg' is your dataframe and 'procedural_stop_words' is your stop word list\n",
        "# First, you need to have a combined list of tokens for each document\n",
        "# Let's assume all_dir_reg now contains a column 'preamble' which is the result of our previous processing\n",
        "\n",
        "# Calculate word frequencies across all documents\n",
        "word_frequencies = Counter()\n",
        "all_dir_reg['preamble'].str.split().apply(word_frequencies.update)\n",
        "\n",
        "total_words = sum(word_frequencies.values())\n",
        "\n",
        "# Set the threshold\n",
        "t = 1e-5\n",
        "\n",
        "# Calculate word probabilities for subsampling\n",
        "word_probs = {word: 1 - np.sqrt(t / (freq / total_words)) for word, freq in word_frequencies.items()}\n",
        "word_probs = {word: max(prob, 0) for word, prob in word_probs.items()}  # Ensure probabilities are non-negative\n",
        "\n",
        "# Function to subsample tokens in a document\n",
        "def subsample_document(tokens, word_probs):\n",
        "    return [token for token in tokens if random.uniform(0, 1) >= word_probs.get(token, 0)]\n",
        "\n",
        "# Apply subsampling to each document\n",
        "all_dir_reg['subsampled_text'] = all_dir_reg['preamble'].apply(lambda text: ' '.join(subsample_document(text.split(), word_probs)))\n"
      ],
      "metadata": {
        "id": "yS6CsnIvSUwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"We split the preambles into segments of 100 words…\""
      ],
      "metadata": {
        "id": "1clNBIZfWzl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define split function\n",
        "def split_into_segments(text, segment_size=100):\n",
        "    if not isinstance(text, str):\n",
        "        return []  # Return an empty list if the input is not a valid string\n",
        "\n",
        "    words = re.split(r'\\s+', text)\n",
        "    segments = [\n",
        "        \" \".join(words[i:i + segment_size])\n",
        "        for i in range(0, len(words), segment_size)\n",
        "    ]\n",
        "    return segments"
      ],
      "metadata": {
        "id": "EJkzbrlvXNj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming all_dir_reg is a pandas DataFrame and 'preamble' is the column with text\n",
        "# all_dir_reg['preamble_segment'] = all_dir_reg['preamble'].apply(split_into_segments)\n",
        "all_dir_reg['preamble_segment'] = all_dir_reg['subsampled_text'].apply(split_into_segments) # Use text with additional preprocessing\n",
        "\n",
        "# Unnest the segments into separate columns\n",
        "max_segments = all_dir_reg['preamble_segment'].apply(len).max()\n",
        "segment_columns = [f'preamble_segment_{i+1}' for i in range(max_segments)]\n",
        "\n",
        "# Expand the list of segments into separate columns\n",
        "preamble_segments_df = pd.DataFrame(all_dir_reg['preamble_segment'].to_list(), columns=segment_columns)\n",
        "all_dir_reg = pd.concat([all_dir_reg, preamble_segments_df], axis=1).drop(columns=['preamble_segment'])\n"
      ],
      "metadata": {
        "id": "NSx7a6HWF1DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Classification\n",
        "\n",
        "RoBERT-classifier trained on the corpus of party manifestos.\n",
        "\n",
        "\"We […] classify each segment as left, neutral, or right\""
      ],
      "metadata": {
        "id": "siRx9Tg1W3qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Hugging Face pipeline\n",
        "RoBERT_classifier = pipeline(\n",
        "    task=\"text-classification\",\n",
        "    model=\"niksmer/RoBERTa-RILE\",\n",
        "    device=0 # CPU: -1 | GPU: 0\n",
        ")"
      ],
      "metadata": {
        "id": "s4KRcNhXIpv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"niksmer/RoBERTa-RILE\")\n",
        "\n",
        "# Fix error: Function to truncate text if it exceeds 512 tokens\n",
        "def truncate_text(text, max_tokens=510): # play it safe\n",
        "    # Tokenize the text\n",
        "    tokens = tokenizer(text, truncation=False)[\"input_ids\"]\n",
        "\n",
        "    # Check if the token length exceeds the max allowed tokens\n",
        "    if len(tokens) > max_tokens:\n",
        "        # Truncate the text to the maximum number of tokens and decode back to string\n",
        "        truncated_text = tokenizer.decode(tokens[:max_tokens], skip_special_tokens=True)\n",
        "        return truncated_text\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# Iterate over the DataFrame and truncate text in relevant columns\n",
        "def truncate_long_segments(df, max_tokens=510): # play it safe\n",
        "    # Iterate over each row\n",
        "    for idx, row in df.iterrows():\n",
        "        # Iterate over each column that starts with \"preamble_segment\"\n",
        "        for col in df.columns:\n",
        "            if col.startswith(\"preamble_segment\"):\n",
        "                original_text = row[col]\n",
        "                if isinstance(original_text, str):  # Ensure it's a string before processing\n",
        "                    # Truncate the text if necessary\n",
        "                    df.at[idx, col] = truncate_text(original_text, max_tokens=max_tokens)\n",
        "    return df\n",
        "\n",
        "# Apply the truncation to the DataFrame\n",
        "all_dir_reg_truncated = truncate_long_segments(all_dir_reg)"
      ],
      "metadata": {
        "id": "IdmcF2xZCP4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the classify_batch function\n",
        "def classify_batch(batch):\n",
        "    return [RoBERT_classifier(text)[0]['label'] for text in batch[\"text\"]]\n",
        "\n",
        "# Create a dataset from the DataFrame\n",
        "def prepare_dataset(df):\n",
        "    # Flatten segments into a list with associated indices\n",
        "    data = []\n",
        "    for idx, row in df.iterrows():\n",
        "        segments = [row[col] for col in row.index if col.startswith('preamble_segment')]\n",
        "        segments = [seg for seg in segments if isinstance(seg, str)]  # Ensure the segment is a string\n",
        "        for segment in segments:\n",
        "            data.append({\"idx\": idx, \"text\": segment})\n",
        "    return Dataset.from_list(data)\n",
        "\n",
        "# Prepare the dataset for classification\n",
        "#dataset = prepare_dataset(all_dir_reg)\n",
        "dataset = prepare_dataset(all_dir_reg_truncated)\n",
        "\n",
        "# Set the desired batch size\n",
        "batch_size = 32  # Adjust this value as needed\n",
        "\n",
        "# Apply the classifier to the dataset with progress display\n",
        "results = []\n",
        "dataset_dict = dataset.to_dict()[\"text\"]\n",
        "for i in tqdm(range(0, len(dataset_dict), batch_size), desc=\"Classifying segments\"):\n",
        "    batch = dataset_dict[i:i + batch_size]\n",
        "    results.extend(classify_batch({\"text\": batch}))\n",
        "\n",
        "# Map the results back to the original DataFrame\n",
        "label_map = {idx: [] for idx in all_dir_reg.index}\n",
        "for item, label in zip(dataset, results):\n",
        "    label_map[item[\"idx\"]].append(label)\n",
        "\n",
        "# Create the final DataFrame\n",
        "RoBERT_df = pd.DataFrame({\n",
        "    'CELEX': all_dir_reg['CELEX'],\n",
        "    'RoBERT_rile_labels': [\", \".join(label_map[idx]) for idx in all_dir_reg.index]\n",
        "})"
      ],
      "metadata": {
        "id": "kmgRGFZwOMHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RoBERT_df.to_csv(\"/content/drive/MyDrive/EU Policy Feedback/RoBERT_df_add_preprocessing.csv\", index=False)"
      ],
      "metadata": {
        "id": "kFoevJIPI8fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ManiBERT\n",
        "Classifier fine-tuned to identify the Comparative Manifesto Project (CMP) policy-issue codes"
      ],
      "metadata": {
        "id": "KA3N0ZAjW_IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Hugging Face pipeline for ManiBERT with GPU utilization\n",
        "ManiBERT_classifier = pipeline(\n",
        "    task=\"text-classification\",\n",
        "    model=\"niksmer/ManiBERT\",\n",
        "    device=0 # CPU: -1 | GPU: 0\n",
        ")"
      ],
      "metadata": {
        "id": "AL63MwSuYJwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select relevant columns and reshape the DataFrame\n",
        "def prepare_maniBERT_dataset(df):\n",
        "    # Reshape the DataFrame: pivot longer and drop NA values\n",
        "    df_long = df.melt(id_vars=[\"CELEX\"], value_vars=[col for col in df.columns if col.startswith(\"preamble_segment\")],\n",
        "                      var_name=\"segment\", value_name=\"text\").dropna(subset=[\"text\"])\n",
        "\n",
        "    # Convert the reshaped DataFrame to a Hugging Face Dataset\n",
        "    return Dataset.from_pandas(df_long)\n",
        "\n",
        "# Prepare the dataset\n",
        "maniBERT_dataset = prepare_maniBERT_dataset(all_dir_reg_truncated)\n",
        "\n",
        "# Function to classify text using ManiBERT\n",
        "def classify_text(batch):\n",
        "    return [ManiBERT_classifier(text)[0]['label'] for text in batch['text']]\n",
        "\n",
        "# Apply the classifier to the dataset using batched processing with progress display\n",
        "batch_size = 32  # Adjust this value as needed\n",
        "maniBERT_dataset = maniBERT_dataset.map(lambda batch: {'ManiBERT_label': classify_text(batch)},\n",
        "                                        batched=True,\n",
        "                                        batch_size=batch_size,\n",
        "                                        desc=\"Processing segments\")\n",
        "\n",
        "# Convert the dataset back to a DataFrame\n",
        "ManiBERT_df = maniBERT_dataset.to_pandas()"
      ],
      "metadata": {
        "id": "4z3ro88rXdEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ManiBERT_df.to_csv(\"/content/drive/MyDrive/EU Policy Feedback/ManiBERT_df_add_preprocessing.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "NJTgzmHJYDSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Object Checks"
      ],
      "metadata": {
        "id": "i7QYBJINALZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print head of all_dir_reg\n",
        "\n",
        "#print(all_dir_reg.head())\n",
        "#print(RoBERT_df.shape)\n",
        "#print(RoBERT_df.head())\n",
        "\n",
        "#print(ManiBERT_df.shape)\n",
        "#print(ManiBERT_df.tail())"
      ],
      "metadata": {
        "id": "PWgI93nRFvEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: get size of dataframe\n",
        "\n",
        "#print(all_dir_reg.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "r9qen2PuedlL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}