{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recreate Hix Hoyland (2024)\n",
        "This notebook is based on the script `eu-policy-feedback/existing_measurements/hix_hoyland_2024/hix_hoyland_2024.R` and is optimised to process a large amount (>70k) of EU legislations using Google Colab hardware, e.g., a GPU.\n",
        "\n",
        "\n",
        "Connect to a GPU for best performance."
      ],
      "metadata": {
        "id": "9JsbDJ33WOyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "jEu7c2gvXLN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets tqdm"
      ],
      "metadata": {
        "id": "IBklQmbvPChG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from itertools import islice\n",
        "import math\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from datasets import Dataset\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "-uJFI05QFkKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t3MK1cvYE6Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "id": "ErXV9kWoWd77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_dir_reg = pd.read_csv(\"/content/drive/MyDrive/EU Policy Feedback/all_dir_reg.csv\")\n",
        "#all_dir_reg = pd.read_csv(\"/content/drive/MyDrive/EU Policy Feedback/all_dir_reg_sample.csv\")"
      ],
      "metadata": {
        "id": "eomh3V_6EzLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing\n",
        "\"For each piece of legislation, we classified each sentence in the preamble, until the phrase “Adopted this directive/regulation”, using a RoBERT-classifier trained on the corpus of party manifestos\"\n",
        "\n",
        "Get preamble string until “Adopted this directive/regulation”"
      ],
      "metadata": {
        "id": "LuhbcR-zWWck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract the preamble text\n",
        "def extract_preamble(text):\n",
        "    if not isinstance(text, str):\n",
        "        return None  # or you could return \"\" if you prefer an empty string\n",
        "\n",
        "    # Use a case-insensitive regex to find the first occurrence of the keywords\n",
        "    match = re.search(r'(?i)(Adopted this directive|Adopted this regulation)', text)\n",
        "\n",
        "    # If the keyword is found, truncate the string\n",
        "    if match:\n",
        "        return text[:match.start()]\n",
        "    else:\n",
        "        return text"
      ],
      "metadata": {
        "id": "Pk6ZehmDW25a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming all_dir_reg is a pandas DataFrame and 'act_raw_text' is the column with text\n",
        "all_dir_reg['preamble'] = all_dir_reg['act_raw_text'].apply(extract_preamble)\n"
      ],
      "metadata": {
        "id": "Dxc2xquZFh2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"We split the preambles into segments of 100 words…\""
      ],
      "metadata": {
        "id": "1clNBIZfWzl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define split function\n",
        "def split_into_segments(text, segment_size=100):\n",
        "    if not isinstance(text, str):\n",
        "        return []  # Return an empty list if the input is not a valid string\n",
        "\n",
        "    words = re.split(r'\\s+', text)\n",
        "    segments = [\n",
        "        \" \".join(words[i:i + segment_size])\n",
        "        for i in range(0, len(words), segment_size)\n",
        "    ]\n",
        "    return segments"
      ],
      "metadata": {
        "id": "EJkzbrlvXNj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming all_dir_reg is a pandas DataFrame and 'preamble' is the column with text\n",
        "all_dir_reg['preamble_segment'] = all_dir_reg['preamble'].apply(split_into_segments)\n",
        "\n",
        "# Unnest the segments into separate columns\n",
        "max_segments = all_dir_reg['preamble_segment'].apply(len).max()\n",
        "segment_columns = [f'preamble_segment_{i+1}' for i in range(max_segments)]\n",
        "\n",
        "# Expand the list of segments into separate columns\n",
        "preamble_segments_df = pd.DataFrame(all_dir_reg['preamble_segment'].to_list(), columns=segment_columns)\n",
        "all_dir_reg = pd.concat([all_dir_reg, preamble_segments_df], axis=1).drop(columns=['preamble_segment'])\n"
      ],
      "metadata": {
        "id": "NSx7a6HWF1DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Classification\n",
        "\n",
        "RoBERT-classifier trained on the corpus of party manifestos.\n",
        "\n",
        "\"We […] classify each segment as left, neutral, or right\""
      ],
      "metadata": {
        "id": "siRx9Tg1W3qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Hugging Face pipeline\n",
        "RoBERT_classifier = pipeline(\n",
        "    task=\"text-classification\",\n",
        "    model=\"niksmer/RoBERTa-RILE\",\n",
        "    device=-1  # CPU: -1 | GPU: 0\n",
        ")"
      ],
      "metadata": {
        "id": "s4KRcNhXIpv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"niksmer/RoBERTa-RILE\")\n",
        "\n",
        "# Fix error: Function to truncate text if it exceeds 512 tokens\n",
        "def truncate_text(text, max_tokens=510): # play it safe\n",
        "    # Tokenize the text\n",
        "    tokens = tokenizer(text, truncation=False)[\"input_ids\"]\n",
        "\n",
        "    # Check if the token length exceeds the max allowed tokens\n",
        "    if len(tokens) > max_tokens:\n",
        "        # Truncate the text to the maximum number of tokens and decode back to string\n",
        "        truncated_text = tokenizer.decode(tokens[:max_tokens], skip_special_tokens=True)\n",
        "        return truncated_text\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# Iterate over the DataFrame and truncate text in relevant columns\n",
        "def truncate_long_segments(df, max_tokens=510): # play it safe\n",
        "    # Iterate over each row\n",
        "    for idx, row in df.iterrows():\n",
        "        # Iterate over each column that starts with \"preamble_segment\"\n",
        "        for col in df.columns:\n",
        "            if col.startswith(\"preamble_segment\"):\n",
        "                original_text = row[col]\n",
        "                if isinstance(original_text, str):  # Ensure it's a string before processing\n",
        "                    # Truncate the text if necessary\n",
        "                    df.at[idx, col] = truncate_text(original_text, max_tokens=max_tokens)\n",
        "    return df\n",
        "\n",
        "# Apply the truncation to the DataFrame\n",
        "all_dir_reg_truncated = truncate_long_segments(all_dir_reg)"
      ],
      "metadata": {
        "id": "IdmcF2xZCP4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the classify_batch function\n",
        "def classify_batch(batch):\n",
        "    return [RoBERT_classifier(text)[0]['label'] for text in batch[\"text\"]]\n",
        "\n",
        "# Create a dataset from the DataFrame\n",
        "def prepare_dataset(df):\n",
        "    # Flatten segments into a list with associated indices\n",
        "    data = []\n",
        "    for idx, row in df.iterrows():\n",
        "        segments = [row[col] for col in row.index if col.startswith('preamble_segment')]\n",
        "        segments = [seg for seg in segments if isinstance(seg, str)]  # Ensure the segment is a string\n",
        "        for segment in segments:\n",
        "            data.append({\"idx\": idx, \"text\": segment})\n",
        "    return Dataset.from_list(data)\n",
        "\n",
        "# Prepare the dataset for classification\n",
        "#dataset = prepare_dataset(all_dir_reg)\n",
        "dataset = prepare_dataset(all_dir_reg_truncated)\n",
        "\n",
        "# Set the desired batch size\n",
        "batch_size = 32  # Adjust this value as needed\n",
        "\n",
        "# Apply the classifier to the dataset with progress display\n",
        "results = []\n",
        "dataset_dict = dataset.to_dict()[\"text\"]\n",
        "for i in tqdm(range(0, len(dataset_dict), batch_size), desc=\"Classifying segments\"):\n",
        "    batch = dataset_dict[i:i + batch_size]\n",
        "    results.extend(classify_batch({\"text\": batch}))\n",
        "\n",
        "# Map the results back to the original DataFrame\n",
        "label_map = {idx: [] for idx in all_dir_reg.index}\n",
        "for item, label in zip(dataset, results):\n",
        "    label_map[item[\"idx\"]].append(label)\n",
        "\n",
        "# Create the final DataFrame\n",
        "RoBERT_df = pd.DataFrame({\n",
        "    'CELEX': all_dir_reg['CELEX'],\n",
        "    'RoBERT_rile_labels': [\", \".join(label_map[idx]) for idx in all_dir_reg.index]\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmgRGFZwOMHL",
        "outputId": "6e0f3cc7-5093-4bbc-a2fb-9db1b79e5edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Classifying segments: 100%|██████████| 21/21 [05:09<00:00, 14.72s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RoBERT_df.to_csv(\"/content/drive/MyDrive/EU Policy Feedback/RoBERT_df.csv\", index=False)"
      ],
      "metadata": {
        "id": "kFoevJIPI8fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ManiBERT\n",
        "Classifier fine-tuned to identify the Comparative Manifesto Project (CMP) policy-issue codes"
      ],
      "metadata": {
        "id": "KA3N0ZAjW_IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Hugging Face pipeline for ManiBERT with GPU utilization\n",
        "ManiBERT_classifier = pipeline(\n",
        "    task=\"text-classification\",\n",
        "    model=\"niksmer/ManiBERT\",\n",
        "    device=0  # Use GPU (device 0)\n",
        ")"
      ],
      "metadata": {
        "id": "AL63MwSuYJwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select relevant columns and reshape the DataFrame\n",
        "def prepare_maniBERT_dataset(df):\n",
        "    # Reshape the DataFrame: pivot longer and drop NA values\n",
        "    df_long = df.melt(id_vars=[\"CELEX\"], value_vars=[col for col in df.columns if col.startswith(\"preamble_segment\")],\n",
        "                      var_name=\"segment\", value_name=\"text\").dropna(subset=[\"text\"])\n",
        "\n",
        "    # Convert the reshaped DataFrame to a Hugging Face Dataset\n",
        "    return Dataset.from_pandas(df_long)\n",
        "\n",
        "# Prepare the dataset\n",
        "maniBERT_dataset = prepare_maniBERT_dataset(all_dir_reg)\n",
        "\n",
        "# Function to classify text using ManiBERT\n",
        "def classify_text(batch):\n",
        "    return [ManiBERT_classifier(text)[0]['label'] for text in batch['text']]\n",
        "\n",
        "# Apply the classifier to the dataset using batched processing with progress display\n",
        "batch_size = 32  # Adjust this value as needed\n",
        "maniBERT_dataset = maniBERT_dataset.map(lambda batch: {'ManiBERT_label': classify_text(batch)},\n",
        "                                        batched=True,\n",
        "                                        batch_size=batch_size,\n",
        "                                        desc=\"Processing segments\")\n",
        "\n",
        "# Convert the dataset back to a DataFrame\n",
        "ManiBERT_df = maniBERT_dataset.to_pandas()"
      ],
      "metadata": {
        "id": "4z3ro88rXdEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ManiBERT_df.to_csv(\"/content/drive/MyDrive/EU Policy Feedback/ManiBERT_df.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "NJTgzmHJYDSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Object Checks"
      ],
      "metadata": {
        "id": "i7QYBJINALZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print head of all_dir_reg\n",
        "\n",
        "#print(all_dir_reg.head())\n",
        "print(RoBERT_df.shape)\n",
        "print(RoBERT_df.head())\n",
        "#print(ManiBERT_df.head())\n"
      ],
      "metadata": {
        "id": "PWgI93nRFvEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: get size of dataframe\n",
        "\n",
        "print(all_dir_reg.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "r9qen2PuedlL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}