{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recreate Hix Hoyland (2024)\n",
        "This notebook is based on the script `eu-policy-feedback/existing_measurements/hix_hoyland_2024/hix_hoyland_2024.R` and is optimised to process a large amount (>70k) of EU legislations using Google Colab hardware, e.g., a GPU.\n",
        "\n",
        "\n",
        "Connect to a GPU for best performance."
      ],
      "metadata": {
        "id": "9JsbDJ33WOyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "jEu7c2gvXLN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets tqdm"
      ],
      "metadata": {
        "id": "IBklQmbvPChG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from itertools import islice\n",
        "import math\n",
        "from transformers import pipeline\n",
        "from datasets import Dataset\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "-uJFI05QFkKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t3MK1cvYE6Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "id": "ErXV9kWoWd77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_dir_reg = pd.read_csv(\"/content/drive/MyDrive/EU Policy Feedback/all_dir_reg.csv\")\n",
        "#all_dir_reg = pd.read_csv(\"/content/drive/MyDrive/EU Policy Feedback/all_dir_reg_sample.csv\")"
      ],
      "metadata": {
        "id": "eomh3V_6EzLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing\n",
        "\"For each piece of legislation, we classified each sentence in the preamble, until the phrase “Adopted this directive/regulation”, using a RoBERT-classifier trained on the corpus of party manifestos\"\n",
        "\n",
        "Get preamble string until “Adopted this directive/regulation”"
      ],
      "metadata": {
        "id": "LuhbcR-zWWck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract the preamble text\n",
        "def extract_preamble(text):\n",
        "    if not isinstance(text, str):\n",
        "        return None  # or you could return \"\" if you prefer an empty string\n",
        "\n",
        "    # Use a case-insensitive regex to find the first occurrence of the keywords\n",
        "    match = re.search(r'(?i)(Adopted this directive|Adopted this regulation)', text)\n",
        "\n",
        "    # If the keyword is found, truncate the string\n",
        "    if match:\n",
        "        return text[:match.start()]\n",
        "    else:\n",
        "        return text"
      ],
      "metadata": {
        "id": "Pk6ZehmDW25a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming all_dir_reg is a pandas DataFrame and 'act_raw_text' is the column with text\n",
        "all_dir_reg['preamble'] = all_dir_reg['act_raw_text'].apply(extract_preamble)\n"
      ],
      "metadata": {
        "id": "Dxc2xquZFh2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"We split the preambles into segments of 100 words…\""
      ],
      "metadata": {
        "id": "1clNBIZfWzl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define split function\n",
        "def split_into_segments(text, segment_size=100):\n",
        "    if not isinstance(text, str):\n",
        "        return []  # Return an empty list if the input is not a valid string\n",
        "\n",
        "    words = re.split(r'\\s+', text)\n",
        "    segments = [\n",
        "        \" \".join(words[i:i + segment_size])\n",
        "        for i in range(0, len(words), segment_size)\n",
        "    ]\n",
        "    return segments"
      ],
      "metadata": {
        "id": "EJkzbrlvXNj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming all_dir_reg is a pandas DataFrame and 'preamble' is the column with text\n",
        "all_dir_reg['preamble_segment'] = all_dir_reg['preamble'].apply(split_into_segments)\n",
        "\n",
        "# Unnest the segments into separate columns\n",
        "max_segments = all_dir_reg['preamble_segment'].apply(len).max()\n",
        "segment_columns = [f'preamble_segment_{i+1}' for i in range(max_segments)]\n",
        "\n",
        "# Expand the list of segments into separate columns\n",
        "preamble_segments_df = pd.DataFrame(all_dir_reg['preamble_segment'].to_list(), columns=segment_columns)\n",
        "all_dir_reg = pd.concat([all_dir_reg, preamble_segments_df], axis=1).drop(columns=['preamble_segment'])\n"
      ],
      "metadata": {
        "id": "NSx7a6HWF1DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Classification\n",
        "\n",
        "RoBERT-classifier trained on the corpus of party manifestos.\n",
        "\n",
        "\"We […] classify each segment as left, neutral, or right\""
      ],
      "metadata": {
        "id": "siRx9Tg1W3qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Hugging Face pipeline\n",
        "RoBERT_classifier = pipeline(\n",
        "    task=\"text-classification\",\n",
        "    model=\"niksmer/RoBERTa-RILE\",\n",
        "    device=0  # Use GPU (device 0)\n",
        ")"
      ],
      "metadata": {
        "id": "s4KRcNhXIpv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the classify_batch function\n",
        "def classify_batch(batch):\n",
        "    return [RoBERT_classifier(text)[0]['label'] for text in batch[\"text\"]]\n",
        "\n",
        "# Create a dataset from the DataFrame\n",
        "def prepare_dataset(df):\n",
        "    # Flatten segments into a list with associated indices\n",
        "    data = []\n",
        "    for idx, row in df.iterrows():\n",
        "        segments = [row[col] for col in row.index if col.startswith('preamble_segment')]\n",
        "        segments = [seg for seg in segments if isinstance(seg, str)]  # Ensure the segment is a string\n",
        "        for segment in segments:\n",
        "            data.append({\"idx\": idx, \"text\": segment})\n",
        "    return Dataset.from_list(data)\n",
        "\n",
        "# Prepare the dataset for classification\n",
        "dataset = prepare_dataset(all_dir_reg)\n",
        "\n",
        "# Set the desired batch size\n",
        "batch_size = 32  # Adjust this value as needed\n",
        "\n",
        "# Apply the classifier to the dataset with progress display\n",
        "results = []\n",
        "dataset_dict = dataset.to_dict()[\"text\"]\n",
        "for i in tqdm(range(0, len(dataset_dict), batch_size), desc=\"Classifying segments\"):\n",
        "    batch = dataset_dict[i:i + batch_size]\n",
        "    results.extend(classify_batch({\"text\": batch}))\n",
        "\n",
        "# Map the results back to the original DataFrame\n",
        "label_map = {idx: [] for idx in all_dir_reg.index}\n",
        "for item, label in zip(dataset, results):\n",
        "    label_map[item[\"idx\"]].append(label)\n",
        "\n",
        "# Create the final DataFrame\n",
        "RoBERT_df = pd.DataFrame({\n",
        "    'CELEX': all_dir_reg['CELEX'],\n",
        "    'RoBERT_rile_labels': [\", \".join(label_map[idx]) for idx in all_dir_reg.index]\n",
        "})"
      ],
      "metadata": {
        "id": "kmgRGFZwOMHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RoBERT_df.to_csv(\"/content/drive/MyDrive/EU Policy Feedback/RoBERT_df.csv\", index=False)"
      ],
      "metadata": {
        "id": "kFoevJIPI8fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ManiBERT\n",
        "Classifier fine-tuned to identify the Comparative Manifesto Project (CMP) policy-issue codes"
      ],
      "metadata": {
        "id": "KA3N0ZAjW_IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Hugging Face pipeline for ManiBERT with GPU utilization\n",
        "ManiBERT_classifier = pipeline(\n",
        "    task=\"text-classification\",\n",
        "    model=\"niksmer/ManiBERT\",\n",
        "    device=0  # Use GPU (device 0)\n",
        ")"
      ],
      "metadata": {
        "id": "AL63MwSuYJwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select relevant columns and reshape the DataFrame\n",
        "def prepare_maniBERT_dataset(df):\n",
        "    # Reshape the DataFrame: pivot longer and drop NA values\n",
        "    df_long = df.melt(id_vars=[\"CELEX\"], value_vars=[col for col in df.columns if col.startswith(\"preamble_segment\")],\n",
        "                      var_name=\"segment\", value_name=\"text\").dropna(subset=[\"text\"])\n",
        "\n",
        "    # Convert the reshaped DataFrame to a Hugging Face Dataset\n",
        "    return Dataset.from_pandas(df_long)\n",
        "\n",
        "# Prepare the dataset\n",
        "maniBERT_dataset = prepare_maniBERT_dataset(all_dir_reg)\n",
        "\n",
        "# Function to classify text using ManiBERT\n",
        "def classify_text(batch):\n",
        "    return [ManiBERT_classifier(text)[0]['label'] for text in batch['text']]\n",
        "\n",
        "# Apply the classifier to the dataset using batched processing with progress display\n",
        "batch_size = 32  # Adjust this value as needed\n",
        "maniBERT_dataset = maniBERT_dataset.map(lambda batch: {'ManiBERT_label': classify_text(batch)},\n",
        "                                        batched=True,\n",
        "                                        batch_size=batch_size,\n",
        "                                        desc=\"Processing segments\")\n",
        "\n",
        "# Convert the dataset back to a DataFrame\n",
        "ManiBERT_df = maniBERT_dataset.to_pandas()"
      ],
      "metadata": {
        "id": "4z3ro88rXdEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ManiBERT_df.to_csv(\"/content/drive/MyDrive/EU Policy Feedback/ManiBERT_df.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "NJTgzmHJYDSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Object Checks"
      ],
      "metadata": {
        "id": "i7QYBJINALZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print head of all_dir_reg\n",
        "\n",
        "#print(all_dir_reg.head())\n",
        "print(RoBERT_df.shape)\n",
        "print(RoBERT_df.head())\n",
        "#print(ManiBERT_df.head())\n"
      ],
      "metadata": {
        "id": "PWgI93nRFvEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: get size of dataframe\n",
        "\n",
        "print(all_dir_reg.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "r9qen2PuedlL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}